"""
Multi-Modal Fusion Model - Simplified Version
Combines clinical and image models for final prediction
"""

import numpy as np
import pandas as pd
from pathlib import Path
import joblib
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
import json
from sklearn.metrics import accuracy_score, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class FusionModel:
    def __init__(self):
        self.data_path = Path("ml-pipeline/data")
        self.model_path = Path("ml-pipeline/models")
        
        self.clinical_model = None
        self.scaler = None
        self.results = {}
        
    def load_pretrained_models(self):
        """Load pre-trained clinical model"""
        print("üì¶ Loading pre-trained models...")
        
        # Load clinical model
        try:
            self.clinical_model = joblib.load(self.model_path / 'clinical_ensemble.pkl')
            print("   ‚úÖ Clinical ensemble loaded")
        except:
            try:
                self.clinical_model = joblib.load(self.model_path / 'clinical_random_forest.pkl')
                print("   ‚úÖ Clinical random forest loaded")
            except:
                self.clinical_model = joblib.load(self.model_path / 'clinical_xgboost.pkl')
                print("   ‚úÖ Clinical XGBoost loaded")
        
        # Load scaler
        self.scaler = joblib.load(self.model_path / 'clinical_scaler.pkl')
        print("   ‚úÖ Scaler loaded")
        
        # Note: Image model loading skipped due to format issues
        print("   ‚ö†Ô∏è Image model will use simulated predictions")
    
    def prepare_fusion_data(self):
        """Prepare fusion data"""
        print("\nüìä Preparing fusion data...")
        
        # Load clinical data
        train_clinical = pd.read_csv(self.data_path / 'train' / 'clinical_train.csv')
        val_clinical = pd.read_csv(self.data_path / 'val' / 'clinical_val.csv')
        test_clinical = pd.read_csv(self.data_path / 'test' / 'clinical_test.csv')
        
        # Load image metadata
        train_images = pd.read_csv(self.data_path / 'train' / 'image_metadata_train.csv')
        val_images = pd.read_csv(self.data_path / 'val' / 'image_metadata_val.csv')
        test_images = pd.read_csv(self.data_path / 'test' / 'image_metadata_test.csv')
        
        # Get clinical predictions
        X_train = self.scaler.transform(train_clinical.drop('outcome', axis=1))
        X_val = self.scaler.transform(val_clinical.drop('outcome', axis=1))
        X_test = self.scaler.transform(test_clinical.drop('outcome', axis=1))
        
        self.train_clinical_pred = self.clinical_model.predict_proba(X_train)[:, 1]
        self.val_clinical_pred = self.clinical_model.predict_proba(X_val)[:, 1]
        self.test_clinical_pred = self.clinical_model.predict_proba(X_test)[:, 1]
        
        # Simulate image predictions based on DR grades
        self.train_image_pred = self.simulate_image_predictions(train_images['dr_grade'].values)
        self.val_image_pred = self.simulate_image_predictions(val_images['dr_grade'].values)
        self.test_image_pred = self.simulate_image_predictions(test_images['dr_grade'].values)
        
        # True labels
        self.y_train = train_clinical['outcome'].values
        self.y_val = val_clinical['outcome'].values
        self.y_test = test_clinical['outcome'].values
        
        # Align lengths
        min_train = min(len(self.train_clinical_pred), len(self.train_image_pred))
        min_val = min(len(self.val_clinical_pred), len(self.val_image_pred))
        min_test = min(len(self.test_clinical_pred), len(self.test_image_pred))
        
        self.train_clinical_pred = self.train_clinical_pred[:min_train]
        self.train_image_pred = self.train_image_pred[:min_train]
        self.y_train = self.y_train[:min_train]
        
        self.val_clinical_pred = self.val_clinical_pred[:min_val]
        self.val_image_pred = self.val_image_pred[:min_val]
        self.y_val = self.y_val[:min_val]
        
        self.test_clinical_pred = self.test_clinical_pred[:min_test]
        self.test_image_pred = self.test_image_pred[:min_test]
        self.y_test = self.y_test[:min_test]
        
        print(f"   ‚úÖ Train: {len(self.y_train)} samples")
        print(f"   ‚úÖ Val: {len(self.y_val)} samples")
        print(f"   ‚úÖ Test: {len(self.y_test)} samples")
    
    def simulate_image_predictions(self, grades):
        """Simulate image model predictions from DR grades"""
        # Map DR grades to diabetes probability
        grade_to_prob = {0: 0.15, 1: 0.35, 2: 0.55, 3: 0.75, 4: 0.90}
        base_probs = np.array([grade_to_prob[g] for g in grades])
        
        # Add noise
        noise = np.random.normal(0, 0.1, len(base_probs))
        return np.clip(base_probs + noise, 0, 1)
    
    def evaluate_fusion_approaches(self):
        """Compare different fusion methods"""
        print("\nüìä Evaluating fusion approaches...")
        
        results = {}
        
        # 1. Clinical only
        clinical_pred = (self.test_clinical_pred > 0.5).astype(int)
        results['Clinical Only'] = {
            'accuracy': accuracy_score(self.y_test, clinical_pred),
            'auc': roc_auc_score(self.y_test, self.test_clinical_pred)
        }
        
        # 2. Image only (simulated)
        image_pred = (self.test_image_pred > 0.5).astype(int)
        results['Image Only'] = {
            'accuracy': accuracy_score(self.y_test, image_pred),
            'auc': roc_auc_score(self.y_test, self.test_image_pred)
        }
        
        # 3. Simple average
        avg_prob = (self.test_clinical_pred + self.test_image_pred) / 2
        avg_pred = (avg_prob > 0.5).astype(int)
        results['Simple Average'] = {
            'accuracy': accuracy_score(self.y_test, avg_pred),
            'auc': roc_auc_score(self.y_test, avg_prob)
        }
        
        # 4. Weighted average
        weights = [0.7, 0.3]  # More weight to clinical
        weighted_prob = weights[0] * self.test_clinical_pred + weights[1] * self.test_image_pred
        weighted_pred = (weighted_prob > 0.5).astype(int)
        results['Weighted Average'] = {
            'accuracy': accuracy_score(self.y_test, weighted_pred),
            'auc': roc_auc_score(self.y_test, weighted_prob)
        }
        
        # 5. Maximum
        max_prob = np.maximum(self.test_clinical_pred, self.test_image_pred)
        max_pred = (max_prob > 0.5).astype(int)
        results['Maximum'] = {
            'accuracy': accuracy_score(self.y_test, max_pred),
            'auc': roc_auc_score(self.y_test, max_prob)
        }
        
        # Display results
        print("\n" + "="*50)
        print("FUSION RESULTS")
        print("="*50)
        
        for method, metrics in results.items():
            print(f"\n{method}:")
            print(f"   Accuracy: {metrics['accuracy']:.4f}")
            print(f"   AUC-ROC:  {metrics['auc']:.4f}")
        
        # Plot comparison
        self.plot_results(results)
        
        # Save results
        self.results = results
        
        # Save best fusion config
        best_method = max(results, key=lambda x: results[x]['accuracy'])
        fusion_config = {
            'best_method': best_method,
            'best_accuracy': results[best_method]['accuracy'],
            'best_auc': results[best_method]['auc'],
            'clinical_weight': 0.7 if best_method == 'Weighted Average' else 0.5
        }
        
        with open(self.model_path / 'fusion_config.json', 'w') as f:
            json.dump(fusion_config, f, indent=2)
        
        print("\n" + "="*50)
        print(f"üèÜ BEST METHOD: {best_method}")
        print(f"   Accuracy: {results[best_method]['accuracy']:.4f}")
        print(f"   AUC-ROC:  {results[best_method]['auc']:.4f}")
    
    def plot_results(self, results):
        """Plot comparison of fusion methods"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        methods = list(results.keys())
        accuracies = [results[m]['accuracy'] for m in methods]
        aucs = [results[m]['auc'] for m in methods]
        
        # Accuracy plot
        bars1 = axes[0].bar(range(len(methods)), accuracies, color='steelblue')
        axes[0].set_xticks(range(len(methods)))
        axes[0].set_xticklabels(methods, rotation=45, ha='right')
        axes[0].set_ylabel('Accuracy')
        axes[0].set_title('Accuracy Comparison')
        axes[0].set_ylim([0.5, 1.0])
        
        for bar in bars1:
            height = bar.get_height()
            axes[0].text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.3f}', ha='center', va='bottom')
        
        # AUC plot
        bars2 = axes[1].bar(range(len(methods)), aucs, color='coral')
        axes[1].set_xticks(range(len(methods)))
        axes[1].set_xticklabels(methods, rotation=45, ha='right')
        axes[1].set_ylabel('AUC-ROC')
        axes[1].set_title('AUC-ROC Comparison')
        axes[1].set_ylim([0.5, 1.0])
        
        for bar in bars2:
            height = bar.get_height()
            axes[1].text(bar.get_x() + bar.get_width()/2., height,
                        f'{height:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('docs/fusion_comparison.png')
        plt.close()

def main():
    print("üî¨ Multi-Modal Fusion Training Pipeline")
    print("="*50)
    
    fusion = FusionModel()
    
    # Load models
    fusion.load_pretrained_models()
    
    # Prepare data
    fusion.prepare_fusion_data()
    
    # Evaluate fusion approaches
    fusion.evaluate_fusion_approaches()
    
    print("\n" + "="*50)
    print("‚ú® Fusion evaluation complete!")
    print("\nNext: Run 'python backend/app/main.py' to start the API")

if __name__ == "__main__":
    main()